{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf0de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 09:15:40.944782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-01 09:15:40.944814: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22b270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"\"\n",
    "data_train = pd.read_csv(PATH+\"dataset/vlsp_sentiment_train.csv\", sep='\\t')\n",
    "data_train.columns =['Class', 'Data']\n",
    "data_test = pd.read_csv(PATH+\"dataset/vlsp_sentiment_test.csv\", sep='\\t')\n",
    "data_test.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c85dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 2)\n",
      "(1050, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25dbf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bogo\n",
    "import regex as re\n",
    "import itertools\n",
    "def map_to_unicode():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "def covert_unicode(txt):\n",
    "    dicchar = map_to_unicode()\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "  \n",
    "def preprocess(document):\n",
    "    if type(document) is not str:\n",
    "      document = str(document)\n",
    "    # convert to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    #remove\n",
    "  \n",
    "    # remove html character\n",
    "    document = re.sub(r'<[^>]*>', '', document)\n",
    "\n",
    "    #convert all char to unicode\n",
    "    document = covert_unicode(document)\n",
    "\n",
    "    #remove adjacent identical characters\n",
    "    document = ''.join(c[0] for c in itertools.groupby(document))\n",
    "\n",
    "    #uwf=>ừ,....\n",
    "    document = bogo.process_sequence(document)\n",
    "\n",
    "    # remove error character\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "\n",
    "    # remove multiple space character\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbd4bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       mình đã dùng anywhere thế hệ đầu quả là đầy th...\n",
       "1       quan tâm nhất là độ trễ có cao không dùng thi ...\n",
       "2       dag xài con cùi bắp 98k pin trâu mỗi tội đánh ...\n",
       "3       logitech chắc hàng phải tiền triệu trở lên dùn...\n",
       "4       đang xài con m175 cùi mía nhà xài nhiều chuột ...\n",
       "                              ...                        \n",
       "5095    mình mua máy về đc 1 ngày mà điện thoại khác g...\n",
       "5096    có bạn nào dùng f1ư ko mình dùng m cảm thấy qu...\n",
       "5097    dùng ôp mà bộ nhớ 4gb thì k chơi games đc đâu ...\n",
       "5098    sao tui thích xài hàng ôp mà lựa toàn mấy đứa ...\n",
       "5099    mới mở hộp oy mở vào camera mà đã có ảnh chụp ...\n",
       "Name: Data, Length: 5100, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['Data'] = data_train['Data'].map(lambda x: preprocess(x))\n",
    "data_train['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff566fa",
   "metadata": {},
   "source": [
    "# Teencode-corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531e7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import unidecode\n",
    "import bogo\n",
    "\n",
    "def read_file(file_path):\n",
    "    fi = open(file_path, 'r', encoding='utf-8')\n",
    "    ls = fi.readlines()\n",
    "    return ls\n",
    "\n",
    "'''load dictionary to mapping word'''\n",
    "vowel_file = open(PATH+'correct_teencode_stopword/vietnamese_vowel.json', encoding='utf-8')\n",
    "vowel_dic = json.load(vowel_file)\n",
    "\n",
    "short_word_file = open(PATH+'correct_teencode_stopword/short_word.json', encoding='utf-8')\n",
    "short_word_dic = json.load(short_word_file)\n",
    "\n",
    "single_word_dic = read_file(PATH+'correct_teencode_stopword/unidecode_vietnamese_dic.txt')\n",
    "single_word_dic = [re.sub('\\n','', s) for s in single_word_dic]\n",
    "\n",
    "\n",
    "def replace_one_one(word, dictionary):\n",
    "    new_word = dictionary.get(word,word)\n",
    "    return new_word\n",
    "def correct_vowel(sent, vowel_dictionary):\n",
    "    '''mapping a`, a\\ --> à, ....'''\n",
    "    words = sent.split()\n",
    "    pattern = r'[aăâeêuưiyoôơ][.`~?\\']'\n",
    "    sent = \"\"\n",
    "    for word in words:\n",
    "        p = re.search(pattern, word)\n",
    "        new_word = word\n",
    "        if p:\n",
    "            idx = p.span()\n",
    "            replace_vowel = vowel_dictionary[word[idx[0]]][word[idx[0] + 1]]\n",
    "            new_word = re.sub(pattern, replace_vowel, new_word)\n",
    "        sent += new_word + ' '\n",
    "    return sent\n",
    "\n",
    "def correct_teencode(sent):\n",
    "    sent = preprocess(sent)\n",
    "    sent = correct_vowel(sent, vowel_dic)\n",
    "\n",
    "    words = sent.split()\n",
    "    sent = \"\"\n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        if word[-1] in [',',';']:\n",
    "            new_word = replace_one_one(word[:-1], short_word_dic)\n",
    "            sent += new_word + word[-1]\n",
    "        else:\n",
    "            new_word = replace_one_one(word, short_word_dic)\n",
    "            sent += new_word\n",
    "        sent += ' '\n",
    "    return sent[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b6e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fixteencode = data_train.copy()\n",
    "data_fixteencode['Data'] = data_fixteencode['Data'].map(lambda x: correct_teencode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98f9f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       mình đã dùng anywhere thế hệ đầu quả là đầy th...\n",
       "1       quan tâm nhất là độ trễ có cao không dùng thi ...\n",
       "2       dag xài con cùi bắp 98k pin trâu mỗi tội đánh ...\n",
       "3       logitech chắc hàng phải tiền triệu trở lên dùn...\n",
       "4       đang xài con m175 cùi mía nhà xài nhiều chuột ...\n",
       "                              ...                        \n",
       "5095    mình mua máy về được 1 ngày mà điện thoại khác...\n",
       "5096    có bạn nào dùng f1ư không mình dùng m cảm thấy...\n",
       "5097    dùng ôp mà bộ nhớ 4gb thì không chơi games đượ...\n",
       "5098    sao tui thích xài hàng ôp mà lựa toàn mấy đứa ...\n",
       "5099    mới mở hộp rồi mở vào camera mà đã có ảnh chụp...\n",
       "Name: Data, Length: 5100, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fixteencode['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc108a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.concat([data_fixteencode, data_train])\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "585df5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    2277\n",
       "-1    2125\n",
       " 1    2007\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data_train.drop_duplicates()\n",
    "data_train = data_train.dropna()\n",
    "data_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e2d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "def create_stopwordlist():\n",
    "    f = codecs.open(PATH+'correct_teencode_stopword/vlsp_stopwords.txt', encoding='utf-8')\n",
    "    data = []\n",
    "    null_data = []\n",
    "    for i, line in enumerate(f):\n",
    "        line = repr(line)\n",
    "        line = line[1:len(line)-3]\n",
    "        data.append(line)\n",
    "    return data\n",
    "stopword_vn = create_stopwordlist()\n",
    "\n",
    "def tokenize(text):\n",
    "    text =  text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return [word for word in word_tokenize(text.lower()) if word not in stopword_vn]\n",
    "tokens = []\n",
    "for i in data_train.iloc[:, 1].values:\n",
    "  tokens.append(tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bbd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokens, size=400, window=4, min_count=3, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b152af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [('to', 0.999903678894043), ('sợ', 0.9999028444290161), ('đừng', 0.9999024868011475), ('đấy', 0.9999024271965027), ('bảo', 0.9999019503593445), ('kiểu', 0.9999017715454102), ('hai', 0.9999016523361206), ('so sánh', 0.9999013543128967), ('đầu', 0.999901294708252), ('bảo hành', 0.9999008774757385)] \n",
      " 0.99954414\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# Find the most simlimar word\n",
    "sims = model.wv.most_similar('ném', topn=10) \n",
    "# The similarity between two word \n",
    "a = model.wv.similarity('con chuột', 'ném')\n",
    "print('\\n',sims,'\\n',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ad41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 0].values\n",
    "reviews = data_train.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f112426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    elif label == 0:\n",
    "        encoded_labels.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "\n",
    "encoded_labels = np.array(encoded_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752bae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd1b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review.split())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d502b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8411d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "987fef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(word_reviews)\n",
    "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47891974",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689c9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (6409, 300)\n",
      "Shape of label train and validation tensor: (6409, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data.shape)\n",
    "print('Shape of label train and validation tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ff2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(PATH+'model cbow/vi-model-CBOW.bin', binary=True)\n",
    "\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44c62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "  if epoch < 4:\n",
    "    return lr\n",
    "  return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c52399",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51f370e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 300, 400)     3126000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 300, 400, 1)  0           ['embedding[1][0]']              \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 298, 1, 100)  120100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 297, 1, 100)  160100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 296, 1, 100)  200100      ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_3[0][0]',        \n",
      "                                                                  'max_pooling2d_4[0][0]',        \n",
      "                                                                  'max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 300)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            903         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,607,203\n",
      "Trainable params: 3,607,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.3 #0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(reshape)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.05))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [tf.keras.callbacks.LearningRateScheduler(scheduler),early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d33f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df81aea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 - 19s - loss: 3.7533 - accuracy: 0.4625 - val_loss: 2.1351 - val_accuracy: 0.4727 - lr: 0.0100 - 19s/epoch - 914ms/step\n",
      "Epoch 2/20\n",
      "21/21 - 18s - loss: 1.5087 - accuracy: 0.5697 - val_loss: 1.0493 - val_accuracy: 0.5429 - lr: 0.0100 - 18s/epoch - 844ms/step\n",
      "Epoch 3/20\n",
      "21/21 - 18s - loss: 0.9605 - accuracy: 0.6191 - val_loss: 0.8611 - val_accuracy: 0.5975 - lr: 0.0100 - 18s/epoch - 844ms/step\n",
      "Epoch 4/20\n",
      "21/21 - 18s - loss: 0.7866 - accuracy: 0.6676 - val_loss: 0.7386 - val_accuracy: 0.6287 - lr: 0.0100 - 18s/epoch - 841ms/step\n",
      "Epoch 5/20\n",
      "21/21 - 18s - loss: 0.6763 - accuracy: 0.7041 - val_loss: 0.6509 - val_accuracy: 0.6888 - lr: 0.0090 - 18s/epoch - 847ms/step\n",
      "Epoch 6/20\n",
      "21/21 - 18s - loss: 0.6714 - accuracy: 0.7277 - val_loss: 0.6857 - val_accuracy: 0.6669 - lr: 0.0082 - 18s/epoch - 841ms/step\n",
      "Epoch 7/20\n",
      "21/21 - 18s - loss: 0.6189 - accuracy: 0.7496 - val_loss: 0.6463 - val_accuracy: 0.6919 - lr: 0.0074 - 18s/epoch - 847ms/step\n",
      "Epoch 8/20\n",
      "21/21 - 18s - loss: 0.5590 - accuracy: 0.7999 - val_loss: 0.6463 - val_accuracy: 0.6396 - lr: 0.0067 - 18s/epoch - 845ms/step\n",
      "Epoch 9/20\n",
      "21/21 - 18s - loss: 0.5459 - accuracy: 0.7794 - val_loss: 0.6019 - val_accuracy: 0.7122 - lr: 0.0061 - 18s/epoch - 836ms/step\n",
      "Epoch 10/20\n",
      "21/21 - 18s - loss: 0.4990 - accuracy: 0.8325 - val_loss: 0.5786 - val_accuracy: 0.7285 - lr: 0.0055 - 18s/epoch - 844ms/step\n",
      "Epoch 11/20\n",
      "21/21 - 18s - loss: 0.4684 - accuracy: 0.8471 - val_loss: 0.5711 - val_accuracy: 0.7293 - lr: 0.0050 - 18s/epoch - 843ms/step\n",
      "Epoch 12/20\n",
      "21/21 - 18s - loss: 0.4520 - accuracy: 0.8576 - val_loss: 0.5570 - val_accuracy: 0.7379 - lr: 0.0045 - 18s/epoch - 849ms/step\n",
      "Epoch 13/20\n",
      "21/21 - 18s - loss: 0.4430 - accuracy: 0.8695 - val_loss: 0.5612 - val_accuracy: 0.7371 - lr: 0.0041 - 18s/epoch - 849ms/step\n",
      "Epoch 14/20\n",
      "21/21 - 17s - loss: 0.4261 - accuracy: 0.8791 - val_loss: 0.5487 - val_accuracy: 0.7465 - lr: 0.0037 - 17s/epoch - 833ms/step\n",
      "Epoch 15/20\n",
      "21/21 - 18s - loss: 0.4108 - accuracy: 0.8886 - val_loss: 0.5399 - val_accuracy: 0.7520 - lr: 0.0033 - 18s/epoch - 846ms/step\n",
      "Epoch 16/20\n",
      "21/21 - 18s - loss: 0.3961 - accuracy: 0.9023 - val_loss: 0.5382 - val_accuracy: 0.7543 - lr: 0.0030 - 18s/epoch - 837ms/step\n",
      "Epoch 17/20\n",
      "21/21 - 18s - loss: 0.3906 - accuracy: 0.9048 - val_loss: 0.5397 - val_accuracy: 0.7441 - lr: 0.0027 - 18s/epoch - 846ms/step\n",
      "Epoch 18/20\n",
      "21/21 - 18s - loss: 0.3860 - accuracy: 0.9027 - val_loss: 0.5363 - val_accuracy: 0.7449 - lr: 0.0025 - 18s/epoch - 839ms/step\n",
      "Epoch 19/20\n",
      "21/21 - 18s - loss: 0.3799 - accuracy: 0.9138 - val_loss: 0.5331 - val_accuracy: 0.7473 - lr: 0.0022 - 18s/epoch - 841ms/step\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b33616610>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=256, callbacks=callbacks_list, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74da865",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b5839b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 400)          3126000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              541696    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               51400     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,719,699\n",
      "Trainable params: 3,719,699\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import BatchNormalization\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.3 #0.5\n",
    "hidden_dims = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "lstm_layer = Bidirectional(LSTM(128,dropout=drop))(embedding)\n",
    "\n",
    "dense_layer = Dense(hidden_dims*2, activation='sigmoid')(lstm_layer)\n",
    "dropout= Dropout(0.2)(dense_layer)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.05))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model_LSTM = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model_LSTM.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_LSTM.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [tf.keras.callbacks.LearningRateScheduler(scheduler),early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f761751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 - 40s - loss: 0.6522 - accuracy: 0.6150 - val_loss: 0.5354 - val_accuracy: 0.6388 - lr: 0.0100 - 40s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "21/21 - 36s - loss: 0.3985 - accuracy: 0.7886 - val_loss: 0.4674 - val_accuracy: 0.7231 - lr: 0.0100 - 36s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "21/21 - 36s - loss: 0.2871 - accuracy: 0.8793 - val_loss: 0.4457 - val_accuracy: 0.7441 - lr: 0.0100 - 36s/epoch - 2s/step\n",
      "Epoch 4/20\n",
      "21/21 - 35s - loss: 0.2310 - accuracy: 0.9222 - val_loss: 0.4537 - val_accuracy: 0.7566 - lr: 0.0100 - 35s/epoch - 2s/step\n",
      "Epoch 5/20\n",
      "21/21 - 35s - loss: 0.2063 - accuracy: 0.9388 - val_loss: 0.4526 - val_accuracy: 0.7590 - lr: 0.0090 - 35s/epoch - 2s/step\n",
      "Epoch 6/20\n",
      "21/21 - 35s - loss: 0.1827 - accuracy: 0.9561 - val_loss: 0.4578 - val_accuracy: 0.7566 - lr: 0.0082 - 35s/epoch - 2s/step\n",
      "Epoch 7/20\n",
      "21/21 - 35s - loss: 0.1714 - accuracy: 0.9645 - val_loss: 0.4566 - val_accuracy: 0.7660 - lr: 0.0074 - 35s/epoch - 2s/step\n",
      "Epoch 7: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ae4bb6510>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.fit(X_train, y_train, epochs=20, batch_size=256, callbacks=callbacks_list, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6df4b",
   "metadata": {},
   "source": [
    "# 2 stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d806742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 400)          3126000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 300, 256)         541696    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,832,435\n",
      "Trainable params: 3,832,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, LSTM\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.3 #0.5\n",
    "hidden_dims = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n",
    "\n",
    "lstm_3 = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "lstm_2 = Bidirectional(LSTM(64))(lstm_3)\n",
    "# lstm_1 = Bidirectional(LSTM(256, return_sequences=True))(lstm_2)\n",
    "# lstm_0 = Bidirectional(LSTM(128,dropout=drop))(lstm_1)\n",
    "\n",
    "# dense_layer = Dense(hidden_dims*2, activation='sigmoid')(lstm_0)\n",
    "dropout= Dropout(drop)(lstm_2)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.05))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model_2LSTM = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model_2LSTM.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_2LSTM.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [tf.keras.callbacks.LearningRateScheduler(scheduler),early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a06efb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 - 55s - loss: 0.6340 - accuracy: 0.6914 - val_loss: 0.4875 - val_accuracy: 0.7215 - lr: 0.0100 - 55s/epoch - 3s/step\n",
      "Epoch 2/20\n",
      "21/21 - 47s - loss: 0.2891 - accuracy: 0.9013 - val_loss: 0.4697 - val_accuracy: 0.7488 - lr: 0.0100 - 47s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "21/21 - 47s - loss: 0.2144 - accuracy: 0.9468 - val_loss: 0.4996 - val_accuracy: 0.7434 - lr: 0.0100 - 47s/epoch - 2s/step\n",
      "Epoch 4/20\n",
      "21/21 - 47s - loss: 0.1788 - accuracy: 0.9657 - val_loss: 0.5067 - val_accuracy: 0.7496 - lr: 0.0100 - 47s/epoch - 2s/step\n",
      "Epoch 5/20\n",
      "21/21 - 47s - loss: 0.1580 - accuracy: 0.9768 - val_loss: 0.5161 - val_accuracy: 0.7520 - lr: 0.0090 - 47s/epoch - 2s/step\n",
      "Epoch 6/20\n",
      "21/21 - 47s - loss: 0.1442 - accuracy: 0.9836 - val_loss: 0.5270 - val_accuracy: 0.7512 - lr: 0.0082 - 47s/epoch - 2s/step\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b2ba95610>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2LSTM.fit(X_train, y_train, epochs=20, batch_size=256, callbacks=callbacks_list, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc40196",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75999673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 300)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 300, 400)          3126000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256)              407040    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               51400     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,585,043\n",
      "Trainable params: 3,585,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import BatchNormalization\n",
    "from keras.layers import Bidirectional, GRU\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.3 #0.5\n",
    "hidden_dims = 100\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "lstm_layer = Bidirectional(GRU(128,dropout=drop))(embedding)\n",
    "\n",
    "dense_layer = Dense(hidden_dims*2, activation='sigmoid')(lstm_layer)\n",
    "dropout= Dropout(0.2)(dense_layer)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.05))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model_GRU = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model_GRU.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_GRU.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [tf.keras.callbacks.LearningRateScheduler(scheduler),early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d85740f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 - 33s - loss: 0.6074 - accuracy: 0.6946 - val_loss: 0.4901 - val_accuracy: 0.7184 - lr: 0.0100 - 33s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "21/21 - 29s - loss: 0.2383 - accuracy: 0.9261 - val_loss: 0.5048 - val_accuracy: 0.7402 - lr: 0.0100 - 29s/epoch - 1s/step\n",
      "Epoch 3/20\n",
      "21/21 - 29s - loss: 0.1785 - accuracy: 0.9594 - val_loss: 0.5064 - val_accuracy: 0.7410 - lr: 0.0100 - 29s/epoch - 1s/step\n",
      "Epoch 4/20\n",
      "21/21 - 30s - loss: 0.1686 - accuracy: 0.9626 - val_loss: 0.5018 - val_accuracy: 0.7543 - lr: 0.0100 - 30s/epoch - 1s/step\n",
      "Epoch 5/20\n",
      "21/21 - 30s - loss: 0.1435 - accuracy: 0.9793 - val_loss: 0.4998 - val_accuracy: 0.7613 - lr: 0.0090 - 30s/epoch - 1s/step\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ae533b450>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU.fit(X_train, y_train, epochs=20, batch_size=256, callbacks=callbacks_list, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d9ce2",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04140e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 300, 400)     3126000     ['input_6[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 300, 400, 1)  0           ['embedding[5][0]']              \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 298, 1, 100)  120100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 297, 1, 100)  160100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 296, 1, 100)  200100      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 100)   0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 3, 1, 100)    0           ['max_pooling2d_6[0][0]',        \n",
      "                                                                  'max_pooling2d_7[0][0]',        \n",
      "                                                                  'max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 128)          270848      ['embedding[6][0]']              \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 300)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 428)          0           ['lstm_3[0][0]',                 \n",
      "                                                                  'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 428)          0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3)            1287        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,878,435\n",
      "Trainable params: 3,878,435\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.3 #0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "\n",
    "x = embedding = embedding_layer(inputs)\n",
    "lstm_0 = LSTM(128)(x)\n",
    "\n",
    "cnn_lstm_layer =  concatenate([lstm_0, reshape], axis=1)\n",
    "dropout = Dropout(drop)(cnn_lstm_layer)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.05))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model_cnn_lstm = Model(inputs, output)\n",
    "\n",
    "adam = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model_cnn_lstm.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model_cnn_lstm.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [tf.keras.callbacks.LearningRateScheduler(scheduler),early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76e43a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 09:36:52.302886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n",
      "2022-05-01 09:36:53.985464: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n",
      "2022-05-01 09:36:55.652530: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n",
      "2022-05-01 09:36:57.346746: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n",
      "2022-05-01 09:36:59.057763: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 - 40s - loss: 2.8211 - accuracy: 0.7375 - val_loss: 1.3880 - val_accuracy: 0.7239 - lr: 0.0100 - 40s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "21/21 - 37s - loss: 0.8211 - accuracy: 0.8980 - val_loss: 0.7512 - val_accuracy: 0.7363 - lr: 0.0100 - 37s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "21/21 - 37s - loss: 0.5477 - accuracy: 0.9466 - val_loss: 0.6649 - val_accuracy: 0.7551 - lr: 0.0100 - 37s/epoch - 2s/step\n",
      "Epoch 4/20\n",
      "21/21 - 37s - loss: 0.3586 - accuracy: 0.9440 - val_loss: 0.5873 - val_accuracy: 0.7480 - lr: 0.0100 - 37s/epoch - 2s/step\n",
      "Epoch 5/20\n",
      "21/21 - 37s - loss: 0.2851 - accuracy: 0.9692 - val_loss: 0.5376 - val_accuracy: 0.7512 - lr: 0.0090 - 37s/epoch - 2s/step\n",
      "Epoch 6/20\n",
      "21/21 - 37s - loss: 0.2357 - accuracy: 0.9772 - val_loss: 0.5138 - val_accuracy: 0.7582 - lr: 0.0082 - 37s/epoch - 2s/step\n",
      "Epoch 7/20\n",
      "21/21 - 37s - loss: 0.2083 - accuracy: 0.9830 - val_loss: 0.5225 - val_accuracy: 0.7512 - lr: 0.0074 - 37s/epoch - 2s/step\n",
      "Epoch 8/20\n",
      "21/21 - 37s - loss: 0.1712 - accuracy: 0.9891 - val_loss: 0.5012 - val_accuracy: 0.7590 - lr: 0.0067 - 37s/epoch - 2s/step\n",
      "Epoch 9/20\n",
      "21/21 - 37s - loss: 0.1565 - accuracy: 0.9899 - val_loss: 0.5038 - val_accuracy: 0.7559 - lr: 0.0061 - 37s/epoch - 2s/step\n",
      "Epoch 10/20\n",
      "21/21 - 37s - loss: 0.1440 - accuracy: 0.9926 - val_loss: 0.5025 - val_accuracy: 0.7496 - lr: 0.0055 - 37s/epoch - 2s/step\n",
      "Epoch 11/20\n",
      "21/21 - 37s - loss: 0.1370 - accuracy: 0.9934 - val_loss: 0.5102 - val_accuracy: 0.7488 - lr: 0.0050 - 37s/epoch - 2s/step\n",
      "Epoch 12/20\n",
      "21/21 - 37s - loss: 0.1473 - accuracy: 0.9945 - val_loss: 0.5141 - val_accuracy: 0.7488 - lr: 0.0045 - 37s/epoch - 2s/step\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ae40c9050>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn_lstm.fit(X_train, y_train, epochs=20, batch_size=256, callbacks=callbacks_list, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3824286",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "580e8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 0].values\n",
    "reviews_test = data_test.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b44d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    elif label_test == 0:\n",
    "        encoded_labels_test.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "\n",
    "encoded_labels_test = np.array(encoded_labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89fa93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed_test = []\n",
    "unlabeled_processed_test = [] \n",
    "for review_test in reviews_test:\n",
    "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
    "    reviews_processed_test.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5fad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews_test = []\n",
    "all_words = []\n",
    "for review_test in reviews_processed_test:\n",
    "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
    "    word_reviews_test.append(review_test.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3d9f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6311e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (1050, 300)\n",
      "Shape of label train and validation tensor: (1050, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data_test.shape)\n",
    "print('Shape of label train and validation tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f4377",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e6a3545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cnn = model.predict(data_test)\n",
    "pred = np.argmax(y_pred_cnn, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b81b1622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.argmax(labels_test, axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2aa50ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t 0.6710960927860953\n",
      "Recall:\t\t 0.6714285714285714\n",
      "F1:\t\t 0.6712433781414758\n",
      "Accuracy:\t 0.6714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Precision:\\t\", metrics.precision_score(test, pred, average='weighted',))\n",
    "print(\"Recall:\\t\\t\", metrics.recall_score(test, pred, average='weighted'))\n",
    "print(\"F1:\\t\\t\", metrics.f1_score(test, pred, average='weighted'))\n",
    "print(\"Accuracy:\\t\", metrics.accuracy_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e769d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       350\n",
      "           1       0.63      0.62      0.63       350\n",
      "           2       0.71      0.72      0.71       350\n",
      "\n",
      "    accuracy                           0.67      1050\n",
      "   macro avg       0.67      0.67      0.67      1050\n",
      "weighted avg       0.67      0.67      0.67      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c2c3d",
   "metadata": {},
   "source": [
    "# LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d57afd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 5s 158ms/step - loss: 0.7578 - accuracy: 0.5857\n"
     ]
    }
   ],
   "source": [
    "score = model_LSTM.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbb86feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lstm = model_LSTM.predict(data_test)\n",
    "pred = np.argmax(y_pred_lstm, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c038d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t 0.6179675700322019\n",
      "Recall:\t\t 0.5857142857142857\n",
      "F1:\t\t 0.578965797793387\n",
      "Accuracy:\t 0.5857142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Precision:\\t\", metrics.precision_score(test, pred, average='weighted',))\n",
    "print(\"Recall:\\t\\t\", metrics.recall_score(test, pred, average='weighted'))\n",
    "print(\"F1:\\t\\t\", metrics.f1_score(test, pred, average='weighted'))\n",
    "print(\"Accuracy:\\t\", metrics.accuracy_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9614dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.81      0.62       350\n",
      "           1       0.63      0.43      0.51       350\n",
      "           2       0.72      0.53      0.61       350\n",
      "\n",
      "    accuracy                           0.59      1050\n",
      "   macro avg       0.62      0.59      0.58      1050\n",
      "weighted avg       0.62      0.59      0.58      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3799ff",
   "metadata": {},
   "source": [
    "# Stack lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bae557c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 8s 238ms/step - loss: 0.7425 - accuracy: 0.6305\n"
     ]
    }
   ],
   "source": [
    "score = model_2LSTM.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9995a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2lstm = model_2LSTM.predict(data_test)\n",
    "pred = np.argmax(y_pred_2lstm, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "348740c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t 0.6348759307853005\n",
      "Recall:\t\t 0.6304761904761905\n",
      "F1:\t\t 0.6308252150147337\n",
      "Accuracy:\t 0.6304761904761905\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Precision:\\t\", metrics.precision_score(test, pred, average='weighted',))\n",
    "print(\"Recall:\\t\\t\", metrics.recall_score(test, pred, average='weighted'))\n",
    "print(\"F1:\\t\\t\", metrics.f1_score(test, pred, average='weighted'))\n",
    "print(\"Accuracy:\\t\", metrics.accuracy_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78219760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.58      0.62       350\n",
      "           1       0.57      0.65      0.61       350\n",
      "           2       0.67      0.67      0.67       350\n",
      "\n",
      "    accuracy                           0.63      1050\n",
      "   macro avg       0.63      0.63      0.63      1050\n",
      "weighted avg       0.63      0.63      0.63      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb445ab8",
   "metadata": {},
   "source": [
    "# GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22c001d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 4s 118ms/step - loss: 0.7422 - accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "score = model_GRU.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72bea35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gru = model_GRU.predict(data_test)\n",
    "pred = np.argmax(y_pred_gru, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6d9835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t 0.6248480171740594\n",
      "Recall:\t\t 0.621904761904762\n",
      "F1:\t\t 0.622888664154234\n",
      "Accuracy:\t 0.621904761904762\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\\t\", metrics.precision_score(test, pred, average='weighted',))\n",
    "print(\"Recall:\\t\\t\", metrics.recall_score(test, pred, average='weighted'))\n",
    "print(\"F1:\\t\\t\", metrics.f1_score(test, pred, average='weighted'))\n",
    "print(\"Accuracy:\\t\", metrics.accuracy_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f328e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       350\n",
      "           1       0.56      0.60      0.58       350\n",
      "           2       0.69      0.64      0.66       350\n",
      "\n",
      "    accuracy                           0.62      1050\n",
      "   macro avg       0.62      0.62      0.62      1050\n",
      "weighted avg       0.62      0.62      0.62      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02efd3",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c777f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 5s 150ms/step - loss: 0.6973 - accuracy: 0.6352\n"
     ]
    }
   ],
   "source": [
    "score = model_cnn_lstm.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45fd1eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cnn_lstm = model_cnn_lstm.predict(data_test)\n",
    "pred = np.argmax(y_pred_cnn_lstm, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d4da284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t 0.635171322749185\n",
      "Recall:\t\t 0.6352380952380953\n",
      "F1:\t\t 0.6349293003448344\n",
      "Accuracy:\t 0.6352380952380953\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\\t\", metrics.precision_score(test, pred, average='weighted',))\n",
    "print(\"Recall:\\t\\t\", metrics.recall_score(test, pred, average='weighted'))\n",
    "print(\"F1:\\t\\t\", metrics.f1_score(test, pred, average='weighted'))\n",
    "print(\"Accuracy:\\t\", metrics.accuracy_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c7f0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.63      0.64       350\n",
      "           1       0.58      0.58      0.58       350\n",
      "           2       0.66      0.69      0.68       350\n",
      "\n",
      "    accuracy                           0.64      1050\n",
      "   macro avg       0.64      0.64      0.63      1050\n",
      "weighted avg       0.64      0.64      0.63      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
